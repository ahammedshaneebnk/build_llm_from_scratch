{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7652add8",
   "metadata": {},
   "source": [
    "# GPT Architecture\n",
    "**with placeholder transformer block and layer norm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7dbb35",
   "metadata": {},
   "source": [
    "## Configuration Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6fd946",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of transformer layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c3df6",
   "metadata": {},
   "source": [
    "## GPT Skeleton Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94faf138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23db2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTSkeleton(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super(GGPTSkeleton, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "        self.position_embedding = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "\n",
    "        self.dropout = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "        self.transformer_layers = nn.Seqential(\n",
    "            *[TransformerBlockSkeleton(config) for _ in range(config[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNormSkeleton(config[\"emb_dim\"])\n",
    "\n",
    "        self.out = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_length = token_ids.shape\n",
    "        token_embeds = self.token_embedding(token_ids)\n",
    "        position_embeds = self.position_embedding(torch.arange(seq_length, device=token_ids.device))\n",
    "        x = token_embeds + position_embeds\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer_layers(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75087e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockSkeleton(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super(TransformerBlockSkeleton, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "709d1ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormSkeleton(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps: float = 1e-5):\n",
    "        super(LayerNormSkeleton, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095b977",
   "metadata": {},
   "source": [
    "Recall that nn.Embedding is like a look up table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
