{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc6b413",
   "metadata": {},
   "source": [
    "# Simple Attention Mechanism\n",
    "**Without Trainable Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12d2efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "498277ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c2ca869eb0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 4\n",
    "NUM_TOKENS = 6\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d20ce54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Vector Embeddings:\n",
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
      "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
      "        [ 0.4681, -0.1577,  1.4437,  0.2660],\n",
      "        [ 0.1665,  0.8744, -0.1435, -0.1116],\n",
      "        [ 0.9318,  1.2590,  2.0050,  0.0537],\n",
      "        [ 0.6181, -0.4128, -0.8411, -2.3160]])\n",
      "\n",
      "Shape of Input Vector Embeddings: torch.Size([6, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_embeddings = torch.randn(NUM_TOKENS, EMBEDDING_DIM)\n",
    "\n",
    "print(f\"Input Vector Embeddings:\\n{vector_embeddings}\\n\")\n",
    "\n",
    "print(f\"Shape of Input Vector Embeddings: {vector_embeddings.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de600c36",
   "metadata": {},
   "source": [
    "## For one token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcafba9a",
   "metadata": {},
   "source": [
    "#### Query Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "865f4717",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_VECTOR_INDEX = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d218a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Vector (Index 2):\n",
      "tensor([ 0.4681, -0.1577,  1.4437,  0.2660])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = vector_embeddings[QUERY_VECTOR_INDEX]\n",
    "\n",
    "print(f\"Query Vector (Index {QUERY_VECTOR_INDEX}):\\n{query}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a337e7ad",
   "metadata": {},
   "source": [
    "#### Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd6229e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores:\n",
      "tensor([-0.8224, -0.7308,  2.3989, -0.2968,  3.1464, -1.4760])\n",
      "\n",
      "Shape of Attention Scores: torch.Size([6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attention_scores = torch.empty(NUM_TOKENS)\n",
    "\n",
    "for i in range(NUM_TOKENS):\n",
    "    attention_scores[i] = torch.dot(query, vector_embeddings[i])\n",
    "\n",
    "print(f\"Attention Scores:\\n{attention_scores}\\n\")\n",
    "print(f\"Shape of Attention Scores: {attention_scores.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5bdec",
   "metadata": {},
   "source": [
    "#### Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f9fc5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights by using sum normalization:\n",
      "tensor([-0.3706, -0.3293,  1.0809, -0.1337,  1.4178, -0.6651])\n",
      "\n",
      "Sum of Attention Weights: 0.9999999403953552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalization using just sum\n",
    "attention_weights = attention_scores / torch.sum(attention_scores)\n",
    "\n",
    "print(f\"Attention Weights by using sum normalization:\\n{attention_weights}\\n\")\n",
    "print(f\"Sum of Attention Weights: {torch.sum(attention_weights)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8571e7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights by using naive softmax normalization:\n",
      "tensor([0.0122, 0.0133, 0.3045, 0.0206, 0.6431, 0.0063])\n",
      "\n",
      "Sum of Attention Weights: 0.9999999403953552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalization using naive softmax\n",
    "exp_attention_scores = torch.exp(attention_scores)\n",
    "attention_weights = exp_attention_scores / torch.sum(exp_attention_scores)\n",
    "\n",
    "print(f\"Attention Weights by using naive softmax normalization:\\n{attention_weights}\\n\")\n",
    "print(f\"Sum of Attention Weights: {torch.sum(attention_weights)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2bbbf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights by using PyTorch softmax normalization:\n",
      "tensor([0.0122, 0.0133, 0.3045, 0.0206, 0.6431, 0.0063])\n",
      "\n",
      "Sum of Attention Weights: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalization using PyTorch softmax\n",
    "attention_weights = torch.softmax(attention_scores, dim=0)\n",
    "\n",
    "print(f\"Attention Weights by using PyTorch softmax normalization:\\n{attention_weights}\\n\")\n",
    "print(f\"Sum of Attention Weights: {torch.sum(attention_weights)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb5cbc",
   "metadata": {},
   "source": [
    "#### Context Vector Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49d169e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector:\n",
      "tensor([0.7468, 0.7722, 1.7136, 0.0652])\n",
      "\n",
      "Shape of Context Vector: torch.Size([4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_vector = torch.zeros(EMBEDDING_DIM)\n",
    "for i in range(NUM_TOKENS):\n",
    "    context_vector += attention_weights[i] * vector_embeddings[i]\n",
    "\n",
    "print(f\"Context Vector:\\n{context_vector}\\n\")\n",
    "print(f\"Shape of Context Vector: {context_vector.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4011fa",
   "metadata": {},
   "source": [
    "## All Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32201ec6",
   "metadata": {},
   "source": [
    "#### Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a47fbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores Matrix using for loop:\n",
      "tensor([[ 2.8465, -0.7560, -0.8224, -1.1106, -3.0256,  0.9955],\n",
      "        [-0.7560,  5.7732, -0.7308,  1.0278,  0.9148,  5.4036],\n",
      "        [-0.8224, -0.7308,  2.3989, -0.2968,  3.1464, -1.4760],\n",
      "        [-1.1106,  1.0278, -0.2968,  0.8253,  0.9623,  0.1211],\n",
      "        [-3.0256,  0.9148,  3.1464,  0.9623,  6.4762, -1.7546],\n",
      "        [ 0.9955,  5.4036, -1.4760,  0.1211, -1.7546,  6.6238]])\n",
      "\n",
      "Shape of Attention Scores Matrix: torch.Size([6, 6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using for loop\n",
    "attention_scores = torch.empty(NUM_TOKENS, NUM_TOKENS)\n",
    "\n",
    "for i in range(NUM_TOKENS):\n",
    "    for j in range(NUM_TOKENS):\n",
    "        attention_scores[i, j] = torch.dot(vector_embeddings[i], vector_embeddings[j])\n",
    "\n",
    "print(f\"Attention Scores Matrix using for loop:\\n{attention_scores}\\n\")\n",
    "print(f\"Shape of Attention Scores Matrix: {attention_scores.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce353068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores Matrix without using for loop:\n",
      "tensor([[ 2.8465, -0.7560, -0.8224, -1.1106, -3.0256,  0.9955],\n",
      "        [-0.7560,  5.7732, -0.7308,  1.0278,  0.9148,  5.4036],\n",
      "        [-0.8224, -0.7308,  2.3989, -0.2968,  3.1464, -1.4760],\n",
      "        [-1.1106,  1.0278, -0.2968,  0.8253,  0.9623,  0.1211],\n",
      "        [-3.0256,  0.9148,  3.1464,  0.9623,  6.4762, -1.7546],\n",
      "        [ 0.9955,  5.4036, -1.4760,  0.1211, -1.7546,  6.6238]])\n",
      "\n",
      "Shape of Attention Scores Matrix: torch.Size([6, 6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# without using for loop\n",
    "# this is X * X^T\n",
    "attention_scores = torch.matmul(vector_embeddings, vector_embeddings.T)\n",
    "print(f\"Attention Scores Matrix without using for loop:\\n{attention_scores}\\n\")\n",
    "print(f\"Shape of Attention Scores Matrix: {attention_scores.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "08abc991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores Matrix using @ operator:\n",
      "tensor([[ 2.8465, -0.7560, -0.8224, -1.1106, -3.0256,  0.9955],\n",
      "        [-0.7560,  5.7732, -0.7308,  1.0278,  0.9148,  5.4036],\n",
      "        [-0.8224, -0.7308,  2.3989, -0.2968,  3.1464, -1.4760],\n",
      "        [-1.1106,  1.0278, -0.2968,  0.8253,  0.9623,  0.1211],\n",
      "        [-3.0256,  0.9148,  3.1464,  0.9623,  6.4762, -1.7546],\n",
      "        [ 0.9955,  5.4036, -1.4760,  0.1211, -1.7546,  6.6238]])\n",
      "\n",
      "Shape of Attention Scores Matrix: torch.Size([6, 6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# shortcut for torch.matmul\n",
    "attention_scores = vector_embeddings @ vector_embeddings.T\n",
    "print(f\"Attention Scores Matrix using @ operator:\\n{attention_scores}\\n\")\n",
    "print(f\"Shape of Attention Scores Matrix: {attention_scores.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124dfa68",
   "metadata": {},
   "source": [
    "#### Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a623b5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights Matrix using PyTorch softmax normalization:\n",
      "tensor([[8.1184e-01, 2.2127e-02, 2.0705e-02, 1.5521e-02, 2.2868e-03, 1.2752e-01],\n",
      "        [8.5367e-04, 5.8465e-01, 8.7545e-04, 5.0814e-03, 4.5387e-03, 4.0400e-01],\n",
      "        [1.2151e-02, 1.3317e-02, 3.0454e-01, 2.0555e-02, 6.4311e-01, 6.3212e-03],\n",
      "        [3.3280e-02, 2.8241e-01, 7.5097e-02, 2.3065e-01, 2.6451e-01, 1.1406e-01],\n",
      "        [7.1562e-05, 3.6813e-03, 3.4291e-02, 3.8603e-03, 9.5784e-01, 2.5509e-04],\n",
      "        [2.7633e-03, 2.2691e-01, 2.3339e-04, 1.1526e-03, 1.7664e-04, 7.6876e-01]])\n",
      "\n",
      "Shape of Attention Weights Matrix: torch.Size([6, 6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "\n",
    "print(f\"Attention Weights Matrix using PyTorch softmax normalization:\\n{attention_weights}\\n\")\n",
    "print(f\"Shape of Attention Weights Matrix: {attention_weights.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d13416ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Tensor:\n",
      "tensor([[-1.0886, -0.2666,  0.1894, -0.2190],\n",
      "        [ 2.0576, -0.0354,  0.0627, -0.7663],\n",
      "        [ 1.0993,  2.7565,  0.1753, -0.9315]])\n",
      "Shape of Test Tensor: torch.Size([3, 4])\n",
      "\n",
      "Sum over dim=0:\n",
      "tensor([ 2.0683,  2.4545,  0.4274, -1.9169])\n",
      "Shape of Sum over dim=0: torch.Size([4])\n",
      "\n",
      "Sum over dim=1:\n",
      "tensor([-1.3848,  1.3186,  3.0996])\n",
      "Shape of Sum over dim=1: torch.Size([3])\n",
      "\n",
      "Sum over dim=-1:\n",
      "tensor([-1.3848,  1.3186,  3.0996])\n",
      "Shape of Sum over dim=-1: torch.Size([3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dim argument\n",
    "test_tensor = torch.randn(3, 4)\n",
    "print(f\"Test Tensor:\\n{test_tensor}\")\n",
    "print(f\"Shape of Test Tensor: {test_tensor.shape}\\n\")\n",
    "\n",
    "# sum with dim=0\n",
    "sum_dim0 = torch.sum(test_tensor, dim=0)\n",
    "print(f\"Sum over dim=0:\\n{sum_dim0}\")\n",
    "print(f\"Shape of Sum over dim=0: {sum_dim0.shape}\\n\")\n",
    "\n",
    "# sum with dim=1\n",
    "sum_dim1 = torch.sum(test_tensor, dim=1)\n",
    "print(f\"Sum over dim=1:\\n{sum_dim1}\")\n",
    "print(f\"Shape of Sum over dim=1: {sum_dim1.shape}\\n\")\n",
    "\n",
    "# sum with dim=-1\n",
    "sum_dim_neg1 = torch.sum(test_tensor, dim=-1)\n",
    "print(f\"Sum over dim=-1:\\n{sum_dim_neg1}\")\n",
    "print(f\"Shape of Sum over dim=-1: {sum_dim_neg1.shape}\\n\")\n",
    "\n",
    "# dim=-1 refers to the last dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7337e58",
   "metadata": {},
   "source": [
    "Intuitively, if input is 3x4 then, if we want the o/p to be 3x1 then dim should be 1. If we want the o/p to be 1x4, then dim should be 0. This is true for reduction operations like sum, mean, max etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b1e5b0",
   "metadata": {},
   "source": [
    "#### Context Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93e3eed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vectors:\n",
      "tensor([[-0.8020, -0.9597, -0.2854, -0.6905],\n",
      "        [ 0.7504,  0.2468, -0.5151, -2.1728],\n",
      "        [ 0.7468,  0.7722,  1.7136,  0.0652],\n",
      "        [ 0.5927,  0.6328,  0.4122, -0.8675],\n",
      "        [ 0.9124,  1.2063,  1.9680,  0.0518],\n",
      "        [ 0.6651, -0.1623, -0.7185, -2.2617]])\n",
      "\n",
      "Shape of Context Vectors: torch.Size([6, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_vectors = attention_weights @ vector_embeddings\n",
    "\n",
    "print(f\"Context Vectors:\\n{context_vectors}\\n\")\n",
    "print(f\"Shape of Context Vectors: {context_vectors.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19bc4999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The above is the result of followingg matrix multiplication:\n",
      "\n",
      "Attention Weights Matrix (Shape: torch.Size([6, 6])) @ Vector Embeddings Matrix (Shape: torch.Size([6, 4]))\n",
      "\n",
      "tensor([[8.1184e-01, 2.2127e-02, 2.0705e-02, 1.5521e-02, 2.2868e-03, 1.2752e-01],\n",
      "        [8.5367e-04, 5.8465e-01, 8.7545e-04, 5.0814e-03, 4.5387e-03, 4.0400e-01],\n",
      "        [1.2151e-02, 1.3317e-02, 3.0454e-01, 2.0555e-02, 6.4311e-01, 6.3212e-03],\n",
      "        [3.3280e-02, 2.8241e-01, 7.5097e-02, 2.3065e-01, 2.6451e-01, 1.1406e-01],\n",
      "        [7.1562e-05, 3.6813e-03, 3.4291e-02, 3.8603e-03, 9.5784e-01, 2.5509e-04],\n",
      "        [2.7633e-03, 2.2691e-01, 2.3339e-04, 1.1526e-03, 1.7664e-04, 7.6876e-01]]) \n",
      "*\n",
      " tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
      "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
      "        [ 0.4681, -0.1577,  1.4437,  0.2660],\n",
      "        [ 0.1665,  0.8744, -0.1435, -0.1116],\n",
      "        [ 0.9318,  1.2590,  2.0050,  0.0537],\n",
      "        [ 0.6181, -0.4128, -0.8411, -2.3160]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"The above is the result of followingg matrix multiplication:\\n\")\n",
    "print(f\"Attention Weights Matrix (Shape: {attention_weights.shape}) @ Vector Embeddings Matrix (Shape: {vector_embeddings.shape})\\n\")\n",
    "print(f\"{attention_weights} \\n*\\n {vector_embeddings}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
