{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda4f89f",
   "metadata": {},
   "source": [
    "# Read Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ecc92f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb5ae9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_DIR = \"Resources\"\n",
    "HARRY_POTTER_SS_FILE = \"Harry_Potter_and_Sorcerer's_Stone.txt\"\n",
    "FILE_PATH = os.path.join(RESOURCE_DIR, HARRY_POTTER_SS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b87579ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x93 in position 17968: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(FILE_PATH, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     raw_text = \u001b[43mfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:325\u001b[39m, in \u001b[36mBufferedIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    323\u001b[39m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[32m    324\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.buffer + \u001b[38;5;28minput\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     (result, consumed) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[32m    327\u001b[39m     \u001b[38;5;28mself\u001b[39m.buffer = data[consumed:]\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0x93 in position 17968: invalid start byte"
     ]
    }
   ],
   "source": [
    "with open(FILE_PATH, 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f9679e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE_PATH, 'r', encoding='windows-1252') as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9212745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 442745 characters\n",
      "First 100 characters:\n",
      "Harry Potter and the Sorcerer's Stone \n",
      "\n",
      "CHAPTER ONE \n",
      "\n",
      "THE BOY WHO LIVED \n",
      "\n",
      "Mr. and Mrs. Dursley, of n\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of text: {len(raw_text)} characters\")\n",
    "print(f\"First 100 characters:\\n{raw_text[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb8e3d0",
   "metadata": {},
   "source": [
    "# Python's Regular Expression Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4efd4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c3a8378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens split by whitespace:\n",
      " ['Hello,', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test-text', ' ', 'with', ' ', 'punctuation.']\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Hello, world! This is a test-text with punctuation.\"\n",
    "\n",
    "# split by whitespace alone\n",
    "sample_tokens_whitespace = re.split(r'(\\s)', sample_text)\n",
    "\n",
    "print(\"Tokens split by whitespace:\\n\", sample_tokens_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ec2489a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens split by whitespace and punctuation:\n",
      " ['Hello', ',', '', ' ', 'world', '!', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test-text', ' ', 'with', ' ', 'punctuation', '.', '']\n"
     ]
    }
   ],
   "source": [
    "# split by whitespace, comma, period, and exclamation mark\n",
    "sample_tokens_punct = re.split(r'([.,!]|\\s)', sample_text)\n",
    "\n",
    "print(\"Tokens split by whitespace and punctuation:\\n\", sample_tokens_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "147f0963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Tokens:\n",
      " ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test-text', 'with', 'punctuation', '.']\n"
     ]
    }
   ],
   "source": [
    "cleaned_tokens = [token for token in sample_tokens_punct if token.strip()]\n",
    "\n",
    "print(\"Cleaned Tokens:\\n\", cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c03d274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Tokens with more splitters:\n",
      " ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', '-', 'text', 'with', 'punctuation', '.']\n"
     ]
    }
   ],
   "source": [
    "# more splitters\n",
    "more_splitters = r\"([.,!?\\-;:\\\"'(){}]|--|\\s)\"\n",
    "all_tokens = re.split(more_splitters, sample_text)\n",
    "cleaned_all_tokens = [token.strip() for token in all_tokens if token.strip()]\n",
    "\n",
    "print(\"All Tokens with more splitters:\\n\", cleaned_all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5f914",
   "metadata": {},
   "source": [
    "# Split Raw Text and Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "398feb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 103821\n",
      "First 20 tokens:\n",
      "['Harry', 'Potter', 'and', 'the', 'Sorcerer', \"'\", 's', 'Stone', 'CHAPTER', 'ONE', 'THE', 'BOY', 'WHO', 'LIVED', 'Mr', '.', 'and', 'Mrs', '.', 'Dursley']\n"
     ]
    }
   ],
   "source": [
    "pre_processed_text = re.split(more_splitters, raw_text)\n",
    "tokens = [token.strip() for token in pre_processed_text if token.strip()]\n",
    "\n",
    "print(f\"Total number of tokens: {len(tokens)}\")\n",
    "print(f\"First 20 tokens:\\n{tokens[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef1f28",
   "metadata": {},
   "source": [
    "# Create Vocabulary and IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f7fd617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6669\n",
      "First 30 vocabulary tokens:\n",
      "['!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '1473', '1637', '17', '1709', '1945', '2', '3', '31', '382', '4', '90', ':', ';', '?', 'A', 'AAAAAAAAAARGH', 'AAAARGH', 'ALBUS', 'ALL', 'ALLEY']\n",
      "Last 20 vocabulary tokens:\n",
      "['yesterday', 'yet', 'you', 'young', 'younger', 'youngest', 'youngsters', 'your', 'yours', 'yourself', 'yourselves', 'youth', 'zigzagging', 'zombie', 'zoo', 'zoom', 'zoomed', 'zooming', '–', '“']\n"
     ]
    }
   ],
   "source": [
    "sorted_unique_tokens = sorted(list(set(tokens)))\n",
    "\n",
    "print(f\"Vocabulary size: {len(sorted_unique_tokens)}\")\n",
    "\n",
    "print(f\"First 30 vocabulary tokens:\\n{sorted_unique_tokens[:30]}\")\n",
    "\n",
    "print(f\"Last 20 vocabulary tokens:\\n{sorted_unique_tokens[-20:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d49dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:id for id, token in enumerate(sorted_unique_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc53090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token to ID mapping for first 30 tokens:\n",
      "'!': 0\n",
      "'\"': 1\n",
      "''': 2\n",
      "'(': 3\n",
      "')': 4\n",
      "',': 5\n",
      "'-': 6\n",
      "'.': 7\n",
      "'0': 8\n",
      "'1': 9\n",
      "'1473': 10\n",
      "'1637': 11\n",
      "'17': 12\n",
      "'1709': 13\n",
      "'1945': 14\n",
      "'2': 15\n",
      "'3': 16\n",
      "'31': 17\n",
      "'382': 18\n",
      "'4': 19\n",
      "'90': 20\n",
      "':': 21\n",
      "';': 22\n",
      "'?': 23\n",
      "'A': 24\n",
      "'AAAAAAAAAARGH': 25\n",
      "'AAAARGH': 26\n",
      "'ALBUS': 27\n",
      "'ALL': 28\n",
      "'ALLEY': 29\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token to ID mapping for first 30 tokens:\")\n",
    "for token in sorted_unique_tokens[:30]:\n",
    "    print(f\"'{token}': {vocab[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42463f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token to ID mapping for last 20 tokens:\n",
      "'yesterday': 6649\n",
      "'yet': 6650\n",
      "'you': 6651\n",
      "'young': 6652\n",
      "'younger': 6653\n",
      "'youngest': 6654\n",
      "'youngsters': 6655\n",
      "'your': 6656\n",
      "'yours': 6657\n",
      "'yourself': 6658\n",
      "'yourselves': 6659\n",
      "'youth': 6660\n",
      "'zigzagging': 6661\n",
      "'zombie': 6662\n",
      "'zoo': 6663\n",
      "'zoom': 6664\n",
      "'zoomed': 6665\n",
      "'zooming': 6666\n",
      "'–': 6667\n",
      "'“': 6668\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token to ID mapping for last 20 tokens:\")\n",
    "for token in sorted_unique_tokens[-20:]:\n",
    "    print(f\"'{token}': {vocab[token]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f03a9b2",
   "metadata": {},
   "source": [
    "# Assign Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "021cb7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of token IDs: 103821\n",
      "First 20 token IDs:\n",
      "[571, 967, 1527, 5996, 1150, 2, 5067, 1176, 197, 872, 1206, 102, 1335, 696, 811, 7, 1527, 812, 7, 350]\n",
      "Last 20 token IDs:\n",
      "[2, 4049, 3330, 6088, 3482, 1431, 4017, 4390, 3208, 6560, 344, 6024, 5825, 7, 7, 7, 7, 1, 1206, 354]\n"
     ]
    }
   ],
   "source": [
    "tokenized_output = [vocab[token] for token in tokens]\n",
    "\n",
    "print(f\"Total number of token IDs: {len(tokenized_output)}\")\n",
    "\n",
    "print(f\"First 20 token IDs:\\n{tokenized_output[:20]}\")\n",
    "\n",
    "print(f\"Last 20 token IDs:\\n{tokenized_output[-20:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de8483",
   "metadata": {},
   "source": [
    "# Tokenizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f4730",
   "metadata": {},
   "source": [
    "* Member 1: Vocabulary (token:id map), accepts via contructor\n",
    "* Method 1: Encode (texts -> tokens -> token ids)\n",
    "* Member 2: id:token map (inverse vocabulary), creates inside constructor\n",
    "* Method 2: Decode (token ids -> tokens -> texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocabulary: dict[str, int]):\n",
    "        self.vocab = vocabulary\n",
    "        self.id_to_token = {id: token for token, id in vocabulary.items()}\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        pre_processed = re.split(r\"([.,!?\\-;:\\\"'(){}]|--|\\s)\", text)\n",
    "        tokens = [token.strip() for token in pre_processed if token.strip()]\n",
    "        token_ids = [self.vocab[token] for token in tokens]\n",
    "        return token_ids\n",
    "    def decode(self, token_ids: list[int]) -> str:\n",
    "        tokens = [self.id_to_token[id] for id in token_ids]\n",
    "        text = ' '.join(tokens)\n",
    "        text = re.sub(r'\\s+([.,!?\\-;:\\\"\\'(){}]|--)', r'\\1', text) # replaces space+punctuation with just punctuation\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2be74754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working of join operation:\n",
      "Join directly: HelloShaneebHowareyou?\n",
      "Join with space: Hello Shaneeb How are you ?\n",
      "Join with full stop: Hello.Shaneeb.How.are.you.?\n"
     ]
    }
   ],
   "source": [
    "print(f\"Working of join operation:\")\n",
    "join_list_example = ['Hello', 'Shaneeb', 'How', 'are', 'you', '?']\n",
    "print(f\"Join directly: {''.join(join_list_example)}\")\n",
    "print(f\"Join with space: {' '.join(join_list_example)}\")\n",
    "print(f\"Join with full stop: {'.'.join(join_list_example)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a757935f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded token IDs: [590, 967, 5, 627, 1569, 6651, 2649, 6094, 23]\n"
     ]
    }
   ],
   "source": [
    "new_text = \"Hello Potter, How are you doing today?\"\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "encoded_ids = tokenizer.encode(new_text)\n",
    "print(f\"Encoded token IDs: {encoded_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e56f0c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: Hello Potter, How are you doing today?\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encoded_ids)\n",
    "print(f\"Decoded text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3d4991f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Shaneeb'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m my_text = \u001b[33m\"\u001b[39m\u001b[33mHello Shaneeb, How are you doing today?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m encoded_my_text = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEncoded my text token IDs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoded_my_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      6\u001b[39m pre_processed = re.split(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m([.,!?\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m-;:\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m\"\u001b[39m, text)\n\u001b[32m      7\u001b[39m tokens = [token.strip() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m pre_processed \u001b[38;5;28;01mif\u001b[39;00m token.strip()]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m token_ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m token_ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Shaneeb'"
     ]
    }
   ],
   "source": [
    "my_text = \"Hello Shaneeb, How are you doing today?\"\n",
    "encoded_my_text = tokenizer.encode(my_text)\n",
    "print(f\"Encoded my text token IDs: {encoded_my_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae6a55",
   "metadata": {},
   "source": [
    "# Special Context Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18003ca",
   "metadata": {},
   "source": [
    "We got `KeyError: 'Shaneeb'` because `Shaneeb` is not present in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1515b621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated vocabulary size: 6671\n",
      "Last 10 tokens mapping\n",
      "'zigzagging': 6661\n",
      "'zombie': 6662\n",
      "'zoo': 6663\n",
      "'zoom': 6664\n",
      "'zoomed': 6665\n",
      "'zooming': 6666\n",
      "'–': 6667\n",
      "'“': 6668\n",
      "'<|endoftext|>': 6669\n",
      "'<|unk|>': 6670\n"
     ]
    }
   ],
   "source": [
    "sorted_unique_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:id for id, token in enumerate(sorted_unique_tokens)}\n",
    "\n",
    "print(f\"Updated vocabulary size: {len(vocab)}\")\n",
    "\n",
    "print(f\"Last 10 tokens mapping\")\n",
    "for token in sorted_unique_tokens[-10:]:\n",
    "    print(f\"'{token}': {vocab[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b84fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocabulary: dict[str, int]):\n",
    "        self.vocab = vocabulary\n",
    "        self.id_to_token = {id: token for token, id in vocabulary.items()}\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        pre_processed = re.split(r\"([.,!?\\-;:\\\"'(){}]|--|\\s)\", text)\n",
    "        tokens = [token.strip() for token in pre_processed if token.strip()]\n",
    "        tokens = [token if token in self.vocab else \"<|unk|>\" for token in tokens]\n",
    "        token_ids = [self.vocab[token] for token in tokens]\n",
    "        return token_ids\n",
    "    def decode(self, token_ids: list[int]) -> str:\n",
    "        tokens = [self.id_to_token[id] for id in token_ids]\n",
    "        text = ' '.join(tokens)\n",
    "        text = re.sub(r'\\s+([.,!?\\-;:\\\"\\'(){}]|--)', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "312bedc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded my text token IDs with V2: [590, 6670, 5, 627, 1569, 6651, 2649, 6094, 23]\n"
     ]
    }
   ],
   "source": [
    "tokenizer2 = SimpleTokenizerV2(vocab)\n",
    "encoded_my_text_v2 = tokenizer2.encode(my_text)\n",
    "print(f\"Encoded my text token IDs with V2: {encoded_my_text_v2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9fe5121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded my text from V2 token IDs: Hello <|unk|>, How are you doing today?\n"
     ]
    }
   ],
   "source": [
    "decoded_my_text_v2 = tokenizer2.decode(encoded_my_text_v2)\n",
    "print(f\"Decoded my text from V2 token IDs: {decoded_my_text_v2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47517466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All text with endoftext token:\n",
      "Programming is fun! <|endoftext|> Physics is interesting.\n"
     ]
    }
   ],
   "source": [
    "text_source_1 = \"Programming is fun!\"\n",
    "text_source_2 = \"Physics is interesting.\"\n",
    "\n",
    "all_text = \" <|endoftext|> \".join([text_source_1, text_source_2])\n",
    "print(f\"All text with endoftext token:\\n{all_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "328e5a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded token IDs for all text:\n",
      "[6670, 3739, 3208, 0, 6669, 6670, 3739, 3725, 7]\n"
     ]
    }
   ],
   "source": [
    "encoded_ids_all_text = tokenizer2.encode(all_text)\n",
    "print(f\"Encoded token IDs for all text:\\n{encoded_ids_all_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c88b1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text from all token IDs:\n",
      "<|unk|> is fun! <|endoftext|> <|unk|> is interesting.\n"
     ]
    }
   ],
   "source": [
    "decoded_text_all = tokenizer2.decode(encoded_ids_all_text)\n",
    "print(f\"Decoded text from all token IDs:\\n{decoded_text_all}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ffda6",
   "metadata": {},
   "source": [
    "# Steps\n",
    "* Read raw text with correct encoding\n",
    "* Split with whitespaces, and punctuations if it makes sense\n",
    "* Strip to remove extra whitespaces\n",
    "* Make unique list of tokens (set)\n",
    "* Sort unique tokens in the alphabetical order\n",
    "* Add special context tokens\n",
    "* Assign ids and create vocabulary dict\n",
    "* Create inverse mapping of vocab dict\n",
    "* Accepts new text\n",
    "* Split, strip, and assign id based on vocab dict if present else provide unk id\n",
    "* For decoding, accepts list of ids, get token from inv map, join, clean text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
