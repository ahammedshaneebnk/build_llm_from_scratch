{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda4f89f",
   "metadata": {},
   "source": [
    "# Read Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ecc92f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb5ae9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_DIR = \"Resources\"\n",
    "HARRY_POTTER_SS_FILE = \"Harry_Potter_and_Sorcerer's_Stone.txt\"\n",
    "FILE_PATH = os.path.join(RESOURCE_DIR, HARRY_POTTER_SS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b87579ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x93 in position 17968: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(FILE_PATH, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     raw_text = \u001b[43mfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:325\u001b[39m, in \u001b[36mBufferedIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    323\u001b[39m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[32m    324\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.buffer + \u001b[38;5;28minput\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     (result, consumed) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[32m    327\u001b[39m     \u001b[38;5;28mself\u001b[39m.buffer = data[consumed:]\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0x93 in position 17968: invalid start byte"
     ]
    }
   ],
   "source": [
    "with open(FILE_PATH, 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f9679e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE_PATH, 'r', encoding='windows-1252') as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9212745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 442745 characters\n",
      "First 100 characters:\n",
      "Harry Potter and the Sorcerer's Stone \n",
      "\n",
      "CHAPTER ONE \n",
      "\n",
      "THE BOY WHO LIVED \n",
      "\n",
      "Mr. and Mrs. Dursley, of n\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of text: {len(raw_text)} characters\")\n",
    "print(f\"First 100 characters:\\n{raw_text[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb8e3d0",
   "metadata": {},
   "source": [
    "# Python's Regular Expression Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4efd4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c3a8378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens split by whitespace:\n",
      " ['Hello,', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test-text', ' ', 'with', ' ', 'punctuation.']\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Hello, world! This is a test-text with punctuation.\"\n",
    "\n",
    "# split by whitespace alone\n",
    "sample_tokens_whitespace = re.split(r'(\\s)', sample_text)\n",
    "\n",
    "print(\"Tokens split by whitespace:\\n\", sample_tokens_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ec2489a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens split by whitespace and punctuation:\n",
      " ['Hello', ',', '', ' ', 'world', '!', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test-text', ' ', 'with', ' ', 'punctuation', '.', '']\n"
     ]
    }
   ],
   "source": [
    "# split by whitespace, comma, period, and exclamation mark\n",
    "sample_tokens_punct = re.split(r'([.,!]|\\s)', sample_text)\n",
    "\n",
    "print(\"Tokens split by whitespace and punctuation:\\n\", sample_tokens_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "147f0963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Tokens:\n",
      " ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test-text', 'with', 'punctuation', '.']\n"
     ]
    }
   ],
   "source": [
    "cleaned_tokens = [token for token in sample_tokens_punct if token.strip()]\n",
    "\n",
    "print(\"Cleaned Tokens:\\n\", cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c03d274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Tokens with more splitters:\n",
      " ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', '-', 'text', 'with', 'punctuation', '.']\n"
     ]
    }
   ],
   "source": [
    "# more splitters\n",
    "more_splitters = r\"([.,!?\\-;:\\\"'(){}]|--|\\s)\"\n",
    "all_tokens = re.split(more_splitters, sample_text)\n",
    "cleaned_all_tokens = [token.strip() for token in all_tokens if token.strip()]\n",
    "\n",
    "print(\"All Tokens with more splitters:\\n\", cleaned_all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5f914",
   "metadata": {},
   "source": [
    "# Split Raw Text and Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "398feb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 103821\n",
      "First 20 tokens:\n",
      "['Harry', 'Potter', 'and', 'the', 'Sorcerer', \"'\", 's', 'Stone', 'CHAPTER', 'ONE', 'THE', 'BOY', 'WHO', 'LIVED', 'Mr', '.', 'and', 'Mrs', '.', 'Dursley']\n"
     ]
    }
   ],
   "source": [
    "pre_processed_text = re.split(more_splitters, raw_text)\n",
    "tokens = [token.strip() for token in pre_processed_text if token.strip()]\n",
    "\n",
    "print(f\"Total number of tokens: {len(tokens)}\")\n",
    "print(f\"First 20 tokens:\\n{tokens[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef1f28",
   "metadata": {},
   "source": [
    "# Create Vocabulary and IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f7fd617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6669\n",
      "First 30 vocabulary tokens:\n",
      "['!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '1473', '1637', '17', '1709', '1945', '2', '3', '31', '382', '4', '90', ':', ';', '?', 'A', 'AAAAAAAAAARGH', 'AAAARGH', 'ALBUS', 'ALL', 'ALLEY']\n",
      "Last 20 vocabulary tokens:\n",
      "['yesterday', 'yet', 'you', 'young', 'younger', 'youngest', 'youngsters', 'your', 'yours', 'yourself', 'yourselves', 'youth', 'zigzagging', 'zombie', 'zoo', 'zoom', 'zoomed', 'zooming', '–', '“']\n"
     ]
    }
   ],
   "source": [
    "sorted_unique_tokens = sorted(list(set(tokens)))\n",
    "\n",
    "print(f\"Vocabulary size: {len(sorted_unique_tokens)}\")\n",
    "\n",
    "print(f\"First 30 vocabulary tokens:\\n{sorted_unique_tokens[:30]}\")\n",
    "\n",
    "print(f\"Last 20 vocabulary tokens:\\n{sorted_unique_tokens[-20:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d49dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:id for id, token in enumerate(sorted_unique_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc53090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token to ID mapping for first 30 tokens:\n",
      "'!': 0\n",
      "'\"': 1\n",
      "''': 2\n",
      "'(': 3\n",
      "')': 4\n",
      "',': 5\n",
      "'-': 6\n",
      "'.': 7\n",
      "'0': 8\n",
      "'1': 9\n",
      "'1473': 10\n",
      "'1637': 11\n",
      "'17': 12\n",
      "'1709': 13\n",
      "'1945': 14\n",
      "'2': 15\n",
      "'3': 16\n",
      "'31': 17\n",
      "'382': 18\n",
      "'4': 19\n",
      "'90': 20\n",
      "':': 21\n",
      "';': 22\n",
      "'?': 23\n",
      "'A': 24\n",
      "'AAAAAAAAAARGH': 25\n",
      "'AAAARGH': 26\n",
      "'ALBUS': 27\n",
      "'ALL': 28\n",
      "'ALLEY': 29\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token to ID mapping for first 30 tokens:\")\n",
    "for token in sorted_unique_tokens[:30]:\n",
    "    print(f\"'{token}': {vocab[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42463f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token to ID mapping for last 20 tokens:\n",
      "'yesterday': 6649\n",
      "'yet': 6650\n",
      "'you': 6651\n",
      "'young': 6652\n",
      "'younger': 6653\n",
      "'youngest': 6654\n",
      "'youngsters': 6655\n",
      "'your': 6656\n",
      "'yours': 6657\n",
      "'yourself': 6658\n",
      "'yourselves': 6659\n",
      "'youth': 6660\n",
      "'zigzagging': 6661\n",
      "'zombie': 6662\n",
      "'zoo': 6663\n",
      "'zoom': 6664\n",
      "'zoomed': 6665\n",
      "'zooming': 6666\n",
      "'–': 6667\n",
      "'“': 6668\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token to ID mapping for last 20 tokens:\")\n",
    "for token in sorted_unique_tokens[-20:]:\n",
    "    print(f\"'{token}': {vocab[token]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f03a9b2",
   "metadata": {},
   "source": [
    "# Assign Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "021cb7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of token IDs: 103821\n",
      "First 20 token IDs:\n",
      "[571, 967, 1527, 5996, 1150, 2, 5067, 1176, 197, 872, 1206, 102, 1335, 696, 811, 7, 1527, 812, 7, 350]\n",
      "Last 20 token IDs:\n",
      "[2, 4049, 3330, 6088, 3482, 1431, 4017, 4390, 3208, 6560, 344, 6024, 5825, 7, 7, 7, 7, 1, 1206, 354]\n"
     ]
    }
   ],
   "source": [
    "tokenized_output = [vocab[token] for token in tokens]\n",
    "\n",
    "print(f\"Total number of token IDs: {len(tokenized_output)}\")\n",
    "\n",
    "print(f\"First 20 token IDs:\\n{tokenized_output[:20]}\")\n",
    "\n",
    "print(f\"Last 20 token IDs:\\n{tokenized_output[-20:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c9a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
