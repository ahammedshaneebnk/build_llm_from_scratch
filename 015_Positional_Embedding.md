# Positional Embeddings

## Background: Token Embedding

**Token embedding** is a crucial Step 3 in the LLM training process, following input text conversion to tokens and assignment of token IDs. Its purpose is to convert token IDs into **vectors** in a higher-dimensional space. This step is essential because it retains the **semantic meaning** and relationship between different words (e.g., *dog* and *puppy*).

## The Need for Positional Information

If only token embedding is used, the resulting vector for a token remains the same regardless of its position in the input sequence. For example, the token vector for "cat" is identical in "The cat sat on the mat" and "On the mat the cat sat," even though the word's position dramatically changes the sentence's meaning. Another example is "A killed B" vs "B killed A".

The core problem is that the embedding layer does not inherently incorporate **positional information** esecially when handled in parallel. Therefore, it is critical to **inject additional position information** to the Large Language Model (LLM) to enable it to understand the **order** and **relationship** between tokens. This ensures more accurate and context-aware predictions.

***

## Types of Positional Embeddings

There are two primary types of positional embeddings:

### Absolute Positional Embedding

* **Mechanism:** For each position in the input sequence, a **unique embedding** is **added** to the token's existing embedding to convey its exact location.
* **Resulting Vector:** The final **input embedding** is the sum of the **Token Embedding** and the **Positional Embedding**.
* **Constraint:** Positional vectors must have the **same dimension** as the original token embeddings to allow for vector addition.
* **Usage:** Generally preferred when the **fixed order of tokens is crucial**, such as for sequence generation. **OpenAI GPT models (e.g., GPT-3, GPT-4)** use absolute positional embeddings where the vector values are optimized during the training process. The original Transformer paper used sinusoidal and cosine formulas to calculate these values.

### Relative Positional Embedding

* **Mechanism:** The emphasis is on the **relative position** or **distance** between tokens, rather than their exact location.
* **Advantage:** The model can generalize better to sequences of varying lengths, even those not seen during training.
* **Usage:** Suitable for tasks like language modeling or the analysis of very **long sequences** where understanding the relationship between distant words is more important than their absolute position.

***

## Implementation of Absolute Positional Embedding

The process of generating the final input embeddings that are fed to the LLM involves three layers:

### 1. Token Embedding Layer

* **Setup:** A **vocabulary size** (e.g., 50,257) and a desired **vector dimension** (e.g., 256) are defined.
* **Process:** An input batch of token IDs (e.g., 8 input sequences, 4 tokens each, resulting in an 8x4 tensor) is passed to the embedding layer lookup table. This generates the **Token Embedding Matrix** with a size of **8x4x256**. Each token ID is now represented by a 256-dimensional vector.

### 2. Positional Embedding Layer

* **Setup:** This layer is created based on the **Context Length** (the maximum number of input tokens the LLM processes at one time, e.g., 4).
* **Process:** Only *four* positional embedding vectors are needed (one for position 1, 2, 3, and 4). This results in a Positional Embedding Matrix of size **4x256**. The values are initially random and optimized during training.

### 3. Final Input Embeddings

* **Calculation:** The **Input Embeddings** are generated by adding the Token Embeddings and the Positional Embeddings:
    $$\text{Input Embeddings} = \text{Token Embeddings} + \text{Positional Embeddings}$$
* **Broadcasting:** The addition is possible due to a **broadcasting operation** where the 4x256 positional vectors are automatically duplicated and applied to all 8 rows of the 8x4x256 token embedding matrix.
* **Result:** The final **Input Embeddings** tensor has the size **8x4x256** and is fed as the training input to the LLM. 
***

## Summary

* Token embedding captures the **semantic meaning** of words but loses **positional information**.
* **Positional embedding** is necessary to inject information about the **order and relationship** of tokens in a sequence to ensure context-aware predictions.
* **Absolute positional embedding** adds a unique vector for the **exact position** and is used by GPT models.
* **Relative positional embedding** focuses on the **distance** between tokens, improving generalization for long sequences.
* The final **Input Embedding** fed to the LLM is the sum of the Token Embedding and the Positional Embedding, incorporating both semantic meaning and word order.