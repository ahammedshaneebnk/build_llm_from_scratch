{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aecea4a",
   "metadata": {},
   "source": [
    "# Self Attention\n",
    "**With trainable weights (Wq, Wk, Wv)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17200f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4eaa961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c3ce671d70>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_IN = 4\n",
    "D_OUT = 3\n",
    "CONTEXT_SIZE = 6\n",
    "NUM_TOKENS = 6\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e8d9df",
   "metadata": {},
   "source": [
    "## Input Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d2daa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings:\n",
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
      "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
      "        [ 0.4681, -0.1577,  1.4437,  0.2660],\n",
      "        [ 0.1665,  0.8744, -0.1435, -0.1116],\n",
      "        [ 0.9318,  1.2590,  2.0050,  0.0537],\n",
      "        [ 0.6181, -0.4128, -0.8411, -2.3160]])\n",
      "\n",
      "torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = torch.randn(NUM_TOKENS, D_IN)\n",
    "print(f\"Input Embeddings:\\n{input_embeddings}\\n\")\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2b7ec",
   "metadata": {},
   "source": [
    "## For One Token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e9b34",
   "metadata": {},
   "source": [
    "### Weight Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6ff33b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focusing on token index: 2\n",
      "\n",
      "Input Embedding:\n",
      "tensor([ 0.4681, -0.1577,  1.4437,  0.2660])\n",
      "\n",
      "torch.Size([4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TOKEN_INDEX = 2  # the token we are focusing on\n",
    "print(f\"Focusing on token index: {TOKEN_INDEX}\\n\")\n",
    "print(f\"Input Embedding:\\n{input_embeddings[TOKEN_INDEX]}\\n\")\n",
    "print(f\"{input_embeddings[TOKEN_INDEX].shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89d46751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query, key, value weight matrices\n",
    "torch.manual_seed(0)\n",
    "Wq = torch.nn.Parameter(torch.randn(D_IN, D_OUT))\n",
    "Wk = torch.nn.Parameter(torch.randn(D_IN, D_OUT))\n",
    "Wv = torch.nn.Parameter(torch.randn(D_IN, D_OUT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4a20cc",
   "metadata": {},
   "source": [
    "### Query, Key, Value Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13ce21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Vector:\n",
      "tensor([ 1.1067,  1.0848, -1.7892], grad_fn=<SqueezeBackward4>)\t Shape: torch.Size([3])\n",
      "\n",
      "Key Vector:\n",
      "tensor([-1.3206, -1.3241,  0.1806], grad_fn=<SqueezeBackward4>)\t Shape: torch.Size([3])\n",
      "\n",
      "Value Vector:\n",
      "tensor([ 1.1077, -1.8903, -0.1447], grad_fn=<SqueezeBackward4>)\t Shape: torch.Size([3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = input_embeddings[TOKEN_INDEX] @ Wq\n",
    "key = input_embeddings[TOKEN_INDEX] @ Wk\n",
    "value = input_embeddings[TOKEN_INDEX] @ Wv\n",
    "\n",
    "print(f\"Query Vector:\\n{query}\\t Shape: {query.shape}\\n\")\n",
    "print(f\"Key Vector:\\n{key}\\t Shape: {key.shape}\\n\")\n",
    "print(f\"Value Vector:\\n{value}\\t Shape: {value.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4926d",
   "metadata": {},
   "source": [
    "### Key and Value Matrices\n",
    "We need full key and value matrices to compute the attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2998eb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Key Vectors:\n",
      "tensor([[ 0.4147, -0.6161,  0.3470],\n",
      "        [-3.4411, -0.9688, -2.4070],\n",
      "        [-1.3206, -1.3241,  0.1806],\n",
      "        [-0.0699, -0.2004,  0.0083],\n",
      "        [-2.3540, -2.6627,  0.2559],\n",
      "        [-3.2032,  0.0066, -2.9222]], grad_fn=<MmBackward0>)\t Shape: torch.Size([6, 3])\n",
      "\n",
      "All Value Vectors:\n",
      "tensor([[ 0.3302, -0.1072,  2.1203],\n",
      "        [-1.6966,  6.1661, -2.2009],\n",
      "        [ 1.1077, -1.8903, -0.1447],\n",
      "        [-0.7948,  0.6281, -0.8995],\n",
      "        [ 0.2518, -1.5995, -1.7082],\n",
      "        [-1.2481,  7.0324, -1.2623]], grad_fn=<MmBackward0>)\t Shape: torch.Size([6, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keys = input_embeddings @ Wk  # (NUM_TOKENS, D_OUT)\n",
    "values = input_embeddings @ Wv  # (NUM_TOKENS, D_OUT)\n",
    "\n",
    "print(f\"All Key Vectors:\\n{keys}\\t Shape: {keys.shape}\\n\")\n",
    "print(f\"All Value Vectors:\\n{values}\\t Shape: {values.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ab50d",
   "metadata": {},
   "source": [
    "### Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e98a7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores (unnormalized):\n",
      "tensor([-0.8304, -0.5525, -3.2210, -0.3096, -5.9513,  1.6908],\n",
      "       grad_fn=<SqueezeBackward4>)\n",
      "Shape: torch.Size([6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attention_scores = query @ keys.T\n",
    "\n",
    "print(f\"Attention Scores (unnormalized):\\n{attention_scores}\\nShape: {attention_scores.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1af676",
   "metadata": {},
   "source": [
    "### Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4ed9f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dk: 3\n",
      "D_OUT: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dk = keys.shape[-1]\n",
    "print(f\"dk: {dk}\\nD_OUT: {D_OUT}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccb741f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights (normalized):\n",
      "tensor([0.1232, 0.1447, 0.0310, 0.1664, 0.0064, 0.5283],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Shape: torch.Size([6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores / (dk ** 0.5), dim=-1)\n",
    "print(f\"Attention Weights (normalized):\\n{attention_weights}\\nShape: {attention_weights.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30a7ac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(attention_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deda3ef",
   "metadata": {},
   "source": [
    "### Context Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64427aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector:\n",
      "tensor([-0.9604,  4.6295, -0.8891], grad_fn=<SqueezeBackward4>)\n",
      "Shape: torch.Size([3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_vector = attention_weights @ values\n",
    "\n",
    "print(f\"Context Vector:\\n{context_vector}\\nShape: {context_vector.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a06f5b5",
   "metadata": {},
   "source": [
    "## All Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5ee64bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06232d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(SelfAttention_v1, self).__init__()\n",
    "        self.Wq = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.Wk = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.Wv = nn.Parameter(torch.randn(d_in, d_out))\n",
    "\n",
    "    def forward(self, input_embeddings):\n",
    "        queries = input_embeddings @ self.Wq\n",
    "        keys = input_embeddings @ self.Wk\n",
    "        values = input_embeddings @ self.Wv\n",
    "\n",
    "        dk = keys.shape[-1] # d_out\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / (dk ** 0.5), dim=-1)\n",
    "        context_vectors = attention_weights @ values\n",
    "\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37bb1756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vectors from SelfAttention_v1:\n",
      "tensor([[ 0.3741, -1.1071, -0.7528],\n",
      "        [-0.7642,  3.6132, -0.4795],\n",
      "        [-0.9604,  4.6295, -0.8891],\n",
      "        [-0.7429,  3.4181, -1.0734],\n",
      "        [-1.2533,  6.6111, -1.2935],\n",
      "        [-0.2899,  1.0370,  0.2127]], grad_fn=<MmBackward0>)\n",
      "Shape: torch.Size([6, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "self_attention_v1 = SelfAttention_v1(D_IN, D_OUT)\n",
    "context_vectors = self_attention_v1(input_embeddings)\n",
    "print(f\"Context Vectors from SelfAttention_v1:\\n{context_vectors}\\nShape: {context_vectors.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920b50f",
   "metadata": {},
   "source": [
    "### Better Initialization with nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9317a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(SelfAttention_v2, self).__init__()\n",
    "        self.Wq = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.Wk = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.Wv = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, input_embeddings):\n",
    "        queries = self.Wq(input_embeddings)\n",
    "        keys = self.Wk(input_embeddings)\n",
    "        values = self.Wv(input_embeddings)\n",
    "\n",
    "        dk = keys.shape[-1] # d_out\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / (dk ** 0.5), dim=-1)\n",
    "        context_vectors = attention_weights @ values\n",
    "\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af8d5902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vectors from SelfAttention_v2:\n",
      "tensor([[ 0.4873, -0.5124,  0.0852],\n",
      "        [ 0.3599, -0.2287,  0.2441],\n",
      "        [ 0.5439, -0.6058, -0.0137],\n",
      "        [ 0.4614, -0.4796,  0.1658],\n",
      "        [ 0.5274, -0.5823,  0.0200],\n",
      "        [ 0.3539, -0.2063,  0.2399]], grad_fn=<MmBackward0>)\n",
      "Shape: torch.Size([6, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "self_attention_v2 = SelfAttention_v2(D_IN, D_OUT)\n",
    "context_vectors_v2 = self_attention_v2(input_embeddings)\n",
    "print(f\"Context Vectors from SelfAttention_v2:\\n{context_vectors_v2}\\nShape: {context_vectors_v2.shape}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
