{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b28ba74",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n",
    "**with weight split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e7e4030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c41f77",
   "metadata": {},
   "source": [
    "## Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f0d7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_IN = 4\n",
    "D_OUT = 4 # final output dimension and not per head\n",
    "CONTEXT_SIZE = 6\n",
    "NUM_TOKENS = 6\n",
    "NUM_BATCH = 1 # for simplicity for now\n",
    "NUM_HEADS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c7ff730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings:\n",
      "tensor([[[-1.4208,  0.5254,  0.0145, -1.0520],\n",
      "         [-1.3243, -0.2664,  2.0347, -0.2241],\n",
      "         [-0.6319,  0.8668,  0.3893, -0.0230],\n",
      "         [ 0.7506,  0.0992, -0.2964,  0.4428],\n",
      "         [ 0.5634, -0.8032, -0.3909,  0.4640],\n",
      "         [ 1.2728,  0.6727, -0.2302, -0.7652]]])\n",
      "Shape: torch.Size([1, 6, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = torch.randn(NUM_BATCH, NUM_TOKENS, D_IN)\n",
    "\n",
    "print(f\"Input Embeddings:\\n{input_embeddings}\\nShape: {input_embeddings.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36924739",
   "metadata": {},
   "source": [
    "## Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6057acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wq:\n",
      "Parameter containing:\n",
      "tensor([[ 0.1118, -0.4581, -0.2810, -0.0504],\n",
      "        [ 0.4673,  0.0952, -0.1767,  0.3812],\n",
      "        [-0.1484, -0.0739,  0.4602,  0.3879],\n",
      "        [ 0.2999,  0.3496, -0.2486, -0.0946]], requires_grad=True)\n",
      "Shape: torch.Size([4, 4])\n",
      "\n",
      "Wk:\n",
      "Parameter containing:\n",
      "tensor([[-0.3001, -0.4434, -0.0820, -0.1382],\n",
      "        [ 0.4897, -0.3804,  0.1761,  0.3176],\n",
      "        [ 0.4293, -0.1651, -0.0434,  0.4178],\n",
      "        [ 0.2808,  0.0722,  0.2693,  0.4605]], requires_grad=True)\n",
      "Shape: torch.Size([4, 4])\n",
      "\n",
      "Wv:\n",
      "Parameter containing:\n",
      "tensor([[-0.1242, -0.0797,  0.1834, -0.4971],\n",
      "        [-0.4361, -0.2864,  0.2599,  0.0864],\n",
      "        [ 0.3415, -0.1870,  0.0523,  0.2140],\n",
      "        [ 0.0108,  0.3178, -0.4126,  0.1115]], requires_grad=True)\n",
      "Shape: torch.Size([4, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Wq = nn.Linear(D_IN, D_OUT, bias=False)\n",
    "Wk = nn.Linear(D_IN, D_OUT, bias=False)\n",
    "Wv = nn.Linear(D_IN, D_OUT, bias=False)\n",
    "\n",
    "print(f\"Wq:\\n{Wq.weight}\\nShape: {Wq.weight.shape}\\n\")\n",
    "print(f\"Wk:\\n{Wk.weight}\\nShape: {Wk.weight.shape}\\n\")\n",
    "print(f\"Wv:\\n{Wv.weight}\\nShape: {Wv.weight.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f154e9",
   "metadata": {},
   "source": [
    "## Q,K,V Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6dcd1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries:\n",
      "tensor([[[-0.3506, -1.0175, -0.2295, -0.1465],\n",
      "         [-0.5866, -1.0891,  1.0656, -0.9749],\n",
      "         [-0.5760, -0.2903,  0.1999,  0.0190],\n",
      "         [ 0.0995,  0.5813, -0.0833,  0.2916],\n",
      "         [ 0.5175,  0.4328, -0.0241, -0.0586],\n",
      "         [-0.0626,  0.4078, -0.6414,  0.7465]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Shape: torch.Size([1, 6, 4])\n",
      "\n",
      "Keys:\n",
      "tensor([[[ 0.3377, -1.2272, -1.1369, -0.8416],\n",
      "         [ 0.3797, -0.2600, -0.7065,  0.0536],\n",
      "         [-0.2234, -0.5780, -0.4410, -0.0206],\n",
      "         [-0.3062,  0.4182,  0.5037,  0.3420],\n",
      "         [ 0.1550,  0.6600,  0.5854,  0.2086],\n",
      "         [-0.5557,  0.0838,  0.1256, -0.0084]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Shape: torch.Size([1, 6, 4])\n",
      "\n",
      "Values:\n",
      "tensor([[[ 0.6602,  0.3820, -0.8079,  0.0283],\n",
      "         [ 0.6702,  1.1632, -0.3440, -0.9635],\n",
      "         [ 0.0922,  0.1265, -0.3625,  0.1055],\n",
      "         [-0.3756, -0.3945,  0.3171,  0.2113],\n",
      "         [-0.3083, -0.0772,  0.4215, -0.0361],\n",
      "         [ 0.1264, -0.8737,  0.1331,  0.2373]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Shape: torch.Size([1, 6, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q = Wq(input_embeddings)\n",
    "K = Wk(input_embeddings)\n",
    "V = Wv(input_embeddings)\n",
    "\n",
    "print(f\"Queries:\\n{Q}\\nShape: {Q.shape}\\n\")\n",
    "print(f\"Keys:\\n{K}\\nShape: {K.shape}\\n\")\n",
    "print(f\"Values:\\n{V}\\nShape: {V.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228426ea",
   "metadata": {},
   "source": [
    "## Head Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e216e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head Dimension: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HEAD_DIM = D_OUT // NUM_HEADS\n",
    "\n",
    "print(f\"Head Dimension: {HEAD_DIM}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28888a01",
   "metadata": {},
   "source": [
    "## Reshape Q,K,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cee7afd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Queries:\n",
      "tensor([[[[-0.3506, -1.0175],\n",
      "          [-0.2295, -0.1465]],\n",
      "\n",
      "         [[-0.5866, -1.0891],\n",
      "          [ 1.0656, -0.9749]],\n",
      "\n",
      "         [[-0.5760, -0.2903],\n",
      "          [ 0.1999,  0.0190]],\n",
      "\n",
      "         [[ 0.0995,  0.5813],\n",
      "          [-0.0833,  0.2916]],\n",
      "\n",
      "         [[ 0.5175,  0.4328],\n",
      "          [-0.0241, -0.0586]],\n",
      "\n",
      "         [[-0.0626,  0.4078],\n",
      "          [-0.6414,  0.7465]]]], grad_fn=<ViewBackward0>)\n",
      "Shape: torch.Size([1, 6, 2, 2])\n",
      "\n",
      "Reshaped Keys:\n",
      "tensor([[[[ 0.3377, -1.2272],\n",
      "          [-1.1369, -0.8416]],\n",
      "\n",
      "         [[ 0.3797, -0.2600],\n",
      "          [-0.7065,  0.0536]],\n",
      "\n",
      "         [[-0.2234, -0.5780],\n",
      "          [-0.4410, -0.0206]],\n",
      "\n",
      "         [[-0.3062,  0.4182],\n",
      "          [ 0.5037,  0.3420]],\n",
      "\n",
      "         [[ 0.1550,  0.6600],\n",
      "          [ 0.5854,  0.2086]],\n",
      "\n",
      "         [[-0.5557,  0.0838],\n",
      "          [ 0.1256, -0.0084]]]], grad_fn=<ViewBackward0>)\n",
      "Shape: torch.Size([1, 6, 2, 2])\n",
      "\n",
      "Reshaped Values:\n",
      "tensor([[[[ 0.6602,  0.3820],\n",
      "          [-0.8079,  0.0283]],\n",
      "\n",
      "         [[ 0.6702,  1.1632],\n",
      "          [-0.3440, -0.9635]],\n",
      "\n",
      "         [[ 0.0922,  0.1265],\n",
      "          [-0.3625,  0.1055]],\n",
      "\n",
      "         [[-0.3756, -0.3945],\n",
      "          [ 0.3171,  0.2113]],\n",
      "\n",
      "         [[-0.3083, -0.0772],\n",
      "          [ 0.4215, -0.0361]],\n",
      "\n",
      "         [[ 0.1264, -0.8737],\n",
      "          [ 0.1331,  0.2373]]]], grad_fn=<ViewBackward0>)\n",
      "Shape: torch.Size([1, 6, 2, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q = Q.view(NUM_BATCH, NUM_TOKENS, NUM_HEADS, HEAD_DIM)\n",
    "K = K.view(NUM_BATCH, NUM_TOKENS, NUM_HEADS, HEAD_DIM)\n",
    "V = V.view(NUM_BATCH, NUM_TOKENS, NUM_HEADS, HEAD_DIM)\n",
    "\n",
    "print(f\"Reshaped Queries:\\n{Q}\\nShape: {Q.shape}\\n\")\n",
    "print(f\"Reshaped Keys:\\n{K}\\nShape: {K.shape}\\n\")\n",
    "print(f\"Reshaped Values:\\n{V}\\nShape: {V.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9672a82",
   "metadata": {},
   "source": [
    "## Regroup Q,K,V into per Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a16406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regrouped Queries:\n",
      "tensor([[[[-0.3506, -1.0175],\n",
      "          [-0.5866, -1.0891],\n",
      "          [-0.5760, -0.2903],\n",
      "          [ 0.0995,  0.5813],\n",
      "          [ 0.5175,  0.4328],\n",
      "          [-0.0626,  0.4078]],\n",
      "\n",
      "         [[-0.2295, -0.1465],\n",
      "          [ 1.0656, -0.9749],\n",
      "          [ 0.1999,  0.0190],\n",
      "          [-0.0833,  0.2916],\n",
      "          [-0.0241, -0.0586],\n",
      "          [-0.6414,  0.7465]]]], grad_fn=<TransposeBackward0>)\n",
      "Shape: torch.Size([1, 2, 6, 2])\n",
      "\n",
      "Regrouped Keys:\n",
      "tensor([[[[ 0.3377, -1.2272],\n",
      "          [ 0.3797, -0.2600],\n",
      "          [-0.2234, -0.5780],\n",
      "          [-0.3062,  0.4182],\n",
      "          [ 0.1550,  0.6600],\n",
      "          [-0.5557,  0.0838]],\n",
      "\n",
      "         [[-1.1369, -0.8416],\n",
      "          [-0.7065,  0.0536],\n",
      "          [-0.4410, -0.0206],\n",
      "          [ 0.5037,  0.3420],\n",
      "          [ 0.5854,  0.2086],\n",
      "          [ 0.1256, -0.0084]]]], grad_fn=<TransposeBackward0>)\n",
      "Shape: torch.Size([1, 2, 6, 2])\n",
      "\n",
      "Regrouped Values:\n",
      "tensor([[[[ 0.6602,  0.3820],\n",
      "          [ 0.6702,  1.1632],\n",
      "          [ 0.0922,  0.1265],\n",
      "          [-0.3756, -0.3945],\n",
      "          [-0.3083, -0.0772],\n",
      "          [ 0.1264, -0.8737]],\n",
      "\n",
      "         [[-0.8079,  0.0283],\n",
      "          [-0.3440, -0.9635],\n",
      "          [-0.3625,  0.1055],\n",
      "          [ 0.3171,  0.2113],\n",
      "          [ 0.4215, -0.0361],\n",
      "          [ 0.1331,  0.2373]]]], grad_fn=<TransposeBackward0>)\n",
      "Shape: torch.Size([1, 2, 6, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q = Q.transpose(1, 2)  # (NUM_BATCH, NUM_HEADS, NUM_TOKENS, HEAD_DIM)\n",
    "K = K.transpose(1, 2)  # (NUM_BATCH, NUM_HEADS, NUM_TOKENS, HEAD_DIM)\n",
    "V = V.transpose(1, 2)  # (NUM_BATCH, NUM_HEADS, NUM_TOKENS, HEAD_DIM)\n",
    "\n",
    "print(f\"Regrouped Queries:\\n{Q}\\nShape: {Q.shape}\\n\")\n",
    "print(f\"Regrouped Keys:\\n{K}\\nShape: {K.shape}\\n\")\n",
    "print(f\"Regrouped Values:\\n{V}\\nShape: {V.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b0967",
   "metadata": {},
   "source": [
    "## Attention Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "44c66ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores:\n",
      "tensor([[[[ 1.1303,  0.1314,  0.6665, -0.3182, -0.7259,  0.1096],\n",
      "          [ 1.1384,  0.0604,  0.7605, -0.2759, -0.8097,  0.2347],\n",
      "          [ 0.1617, -0.1433,  0.2965,  0.0550, -0.2809,  0.2958],\n",
      "          [-0.6798, -0.1134, -0.3582,  0.2127,  0.3991, -0.0065],\n",
      "          [-0.3563,  0.0840, -0.3658,  0.0226,  0.3658, -0.2513],\n",
      "          [-0.5216, -0.1298, -0.2217,  0.1897,  0.2594,  0.0690]],\n",
      "\n",
      "         [[ 0.3842,  0.1543,  0.1042, -0.1657, -0.1649, -0.0276],\n",
      "          [-0.3910, -0.8052, -0.4498,  0.2033,  0.4204,  0.1420],\n",
      "          [-0.2432, -0.1402, -0.0885,  0.1072,  0.1210,  0.0249],\n",
      "          [-0.1507,  0.0745,  0.0307,  0.0578,  0.0120, -0.0129],\n",
      "          [ 0.0767,  0.0139,  0.0118, -0.0322, -0.0263, -0.0025],\n",
      "          [ 0.1009,  0.4932,  0.2674, -0.0677, -0.2197, -0.0868]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "Shape: torch.Size([1, 2, 6, 6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attention_scores = Q @ K.transpose(-2, -1)  # (B, H, T, Dhead) * (B, H, Dhead, T) -> (B, H, T, T)\n",
    "\n",
    "print(f\"Attention Scores:\\n{attention_scores}\\nShape: {attention_scores.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa6c4a",
   "metadata": {},
   "source": [
    "## Negative Infinity Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e818304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "Shape: torch.Size([6, 6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(CONTEXT_SIZE, CONTEXT_SIZE), diagonal=1)\n",
    "\n",
    "print(f\"Mask:\\n{mask}\\nShape: {mask.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1aa9ce61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Attention Scores:\n",
      "tensor([[[[ 1.1303,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 1.1384,  0.0604,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1617, -0.1433,  0.2965,    -inf,    -inf,    -inf],\n",
      "          [-0.6798, -0.1134, -0.3582,  0.2127,    -inf,    -inf],\n",
      "          [-0.3563,  0.0840, -0.3658,  0.0226,  0.3658,    -inf],\n",
      "          [-0.5216, -0.1298, -0.2217,  0.1897,  0.2594,  0.0690]],\n",
      "\n",
      "         [[ 0.3842,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3910, -0.8052,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2432, -0.1402, -0.0885,    -inf,    -inf,    -inf],\n",
      "          [-0.1507,  0.0745,  0.0307,  0.0578,    -inf,    -inf],\n",
      "          [ 0.0767,  0.0139,  0.0118, -0.0322, -0.0263,    -inf],\n",
      "          [ 0.1009,  0.4932,  0.2674, -0.0677, -0.2197, -0.0868]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Shape: torch.Size([1, 2, 6, 6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neg_inf_masked_attention_scores = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "\n",
    "print(f\"Masked Attention Scores:\\n{neg_inf_masked_attention_scores}\\nShape: {neg_inf_masked_attention_scores.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f4c67",
   "metadata": {},
   "source": [
    "## Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac6deb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6818, 0.3182, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3441, 0.2774, 0.3785, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1777, 0.2652, 0.2231, 0.3340, 0.0000, 0.0000],\n",
      "          [0.1579, 0.2156, 0.1569, 0.2064, 0.2632, 0.0000],\n",
      "          [0.1181, 0.1559, 0.1460, 0.1954, 0.2052, 0.1794]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5727, 0.4273, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3134, 0.3370, 0.3496, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2238, 0.2624, 0.2544, 0.2593, 0.0000, 0.0000],\n",
      "          [0.2098, 0.2006, 0.2004, 0.1942, 0.1950, 0.0000],\n",
      "          [0.1665, 0.2198, 0.1873, 0.1478, 0.1327, 0.1458]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Shape: torch.Size([1, 2, 6, 6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(neg_inf_masked_attention_scores / (HEAD_DIM ** 0.5), dim=-1)\n",
    "\n",
    "print(f\"Attention Weights:\\n{attention_weights}\\nShape: {attention_weights.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb2cb81",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b2faf953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights after Dropout:\n",
      "tensor([[[[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8523, 0.3977, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4301, 0.3467, 0.4732, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2221, 0.3315, 0.2788, 0.4175, 0.0000, 0.0000],\n",
      "          [0.1974, 0.2695, 0.1961, 0.0000, 0.3289, 0.0000],\n",
      "          [0.1477, 0.1948, 0.1826, 0.2442, 0.2565, 0.2242]],\n",
      "\n",
      "         [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7159, 0.5341, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3917, 0.0000, 0.4370, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2798, 0.0000, 0.3180, 0.3242, 0.0000, 0.0000],\n",
      "          [0.0000, 0.2508, 0.0000, 0.2428, 0.0000, 0.0000],\n",
      "          [0.2082, 0.2747, 0.2342, 0.0000, 0.1659, 0.1823]]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Shape: torch.Size([1, 2, 6, 6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropout = nn.Dropout(0.2)\n",
    "attention_weights = dropout(attention_weights)\n",
    "\n",
    "print(f\"Attention Weights after Dropout:\\n{attention_weights}\\nShape: {attention_weights.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6044697",
   "metadata": {},
   "source": [
    "## Context Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "60ecadb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vectors:\n",
      "tensor([[[[ 0.8253,  0.4775],\n",
      "          [ 0.8293,  0.7882],\n",
      "          [ 0.5600,  0.6275],\n",
      "          [ 0.2378,  0.3411],\n",
      "          [ 0.2276,  0.3883],\n",
      "          [ 0.1024, -0.0059]],\n",
      "\n",
      "         [[-1.0098,  0.0353],\n",
      "          [-0.7621, -0.4944],\n",
      "          [-0.4748,  0.0572],\n",
      "          [-0.2385,  0.1100],\n",
      "          [-0.0093, -0.1904],\n",
      "          [-0.2533, -0.1968]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Shape: torch.Size([1, 2, 6, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_vectors = attention_weights @ V  # (B, H, T, T) * (B, H, T, Dhead) -> (B, H, T, Dhead)\n",
    "\n",
    "print(f\"Context Vectors:\\n{context_vectors}\\nShape: {context_vectors.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4458dfd",
   "metadata": {},
   "source": [
    "## Regroup Context Vectors into per Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94289cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regrouped Context Vectors:\n",
      "tensor([[[[ 0.8253,  0.4775],\n",
      "          [-1.0098,  0.0353]],\n",
      "\n",
      "         [[ 0.8293,  0.7882],\n",
      "          [-0.7621, -0.4944]],\n",
      "\n",
      "         [[ 0.5600,  0.6275],\n",
      "          [-0.4748,  0.0572]],\n",
      "\n",
      "         [[ 0.2378,  0.3411],\n",
      "          [-0.2385,  0.1100]],\n",
      "\n",
      "         [[ 0.2276,  0.3883],\n",
      "          [-0.0093, -0.1904]],\n",
      "\n",
      "         [[ 0.1024, -0.0059],\n",
      "          [-0.2533, -0.1968]]]], grad_fn=<TransposeBackward0>)\n",
      "Shape: torch.Size([1, 6, 2, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_vectors = context_vectors.transpose(1, 2) # (B, T, H, Dhead)\n",
    "\n",
    "print(f\"Regrouped Context Vectors:\\n{context_vectors}\\nShape: {context_vectors.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf8a3f",
   "metadata": {},
   "source": [
    "## Reshape Context Vectors to Match final D_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41c1e8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Context Vectors:\n",
      "tensor([[[ 0.8253,  0.4775, -1.0098,  0.0353],\n",
      "         [ 0.8293,  0.7882, -0.7621, -0.4944],\n",
      "         [ 0.5600,  0.6275, -0.4748,  0.0572],\n",
      "         [ 0.2378,  0.3411, -0.2385,  0.1100],\n",
      "         [ 0.2276,  0.3883, -0.0093, -0.1904],\n",
      "         [ 0.1024, -0.0059, -0.2533, -0.1968]]], grad_fn=<ViewBackward0>)\n",
      "Shape: torch.Size([1, 6, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_vectors = context_vectors.contiguous().view(NUM_BATCH, NUM_TOKENS, D_OUT)\n",
    "\n",
    "print(f\"Reshaped Context Vectors:\\n{context_vectors}\\nShape: {context_vectors.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6cc81",
   "metadata": {},
   "source": [
    "Why contiguous?\n",
    "* We need to ensure the memory is contiguous before using view().\n",
    "* When a tensor is initialized for the first time, the memory is already contiguous so when view() applied at that time, no need of this.\n",
    "\n",
    "reshape() does not have this condition, then why view() instead of reshape()?\n",
    "* view() ensures zero copy while reshape() does not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ae4b7",
   "metadata": {},
   "source": [
    "## Mult-Head Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0923eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_size, num_heads, dropout_prob):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.Wq = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.Wk = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.Wv = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_size, context_size), diagonal=1))\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "    def forward(self, input_embeddings):\n",
    "        B, T, d_in = input_embeddings.shape\n",
    "\n",
    "        Q = self.Wq(input_embeddings) # (B, T, D_out)\n",
    "        K = self.Wk(input_embeddings) # (B, T, D_out)\n",
    "        V = self.Wv(input_embeddings) # (B, T, D_out)\n",
    "\n",
    "        Q = Q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2) # (B, H, T, Dhead)\n",
    "        K = K.view(B, T, self.num_heads, self.head_dim).transpose(1, 2) # (B, H, T, Dhead)\n",
    "        V = V.view(B, T, self.num_heads, self.head_dim).transpose(1, 2) # (B, H, T, Dhead)\n",
    "\n",
    "        attention_scores = Q @ K.transpose(-2, -1) # (B, H, T, T)\n",
    "        attention_scores = attention_scores.masked_fill(self.mask.bool(), -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores / (self.head_dim ** 0.5), dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        context_vectors = attention_weights @ V # (B, H, T, Dhead)\n",
    "        context_vectors = context_vectors.transpose(1, 2).contiguous().view(B, T, self.d_out) # (B, T, D_out)\n",
    "\n",
    "        output = self.out_proj(context_vectors) # (B, T, D_out)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91af3bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings:\n",
      "tensor([[[ 6.0151e-01,  5.6525e-01, -5.7093e-01, -5.5915e-01],\n",
      "         [-1.5905e+00,  1.0057e+00, -1.1979e+00,  1.6426e+00],\n",
      "         [-1.0055e-01, -8.8790e-01,  1.8286e-01, -6.6268e-01],\n",
      "         [-5.9494e-01,  1.7040e-01, -1.0473e+00, -7.6299e-01],\n",
      "         [-4.8385e-01,  1.2971e-01, -1.0164e+00,  1.0861e+00],\n",
      "         [ 1.7435e+00,  2.9948e-01,  5.3035e-01, -7.9038e-01]],\n",
      "\n",
      "        [[ 2.8088e-01, -1.1303e+00,  1.0223e-01,  7.1892e-01],\n",
      "         [ 2.3506e-04, -1.6781e+00, -1.5704e+00,  6.3134e-01],\n",
      "         [ 1.7100e+00, -1.9451e+00,  3.0664e-01,  3.4074e-01],\n",
      "         [-1.2654e+00, -6.8019e-01, -4.2150e-01,  1.0234e-01],\n",
      "         [ 1.8939e-01,  1.0149e+00, -6.2363e-01, -1.0587e-01],\n",
      "         [-1.4688e+00,  1.3885e+00,  5.7455e-01, -2.1958e-02]]])\n",
      "Shape: torch.Size([2, 6, 4])\n",
      "\n",
      "Multi-Head Attention Output:\n",
      "tensor([[[-0.4930, -0.2375, -0.4020, -0.1955],\n",
      "         [-0.5924, -0.0874, -0.5475, -0.7912],\n",
      "         [-0.4578, -0.1824, -0.4833, -0.5951],\n",
      "         [-0.4172, -0.2191, -0.4720, -0.5743],\n",
      "         [-0.5194, -0.2333, -0.4739, -0.5966],\n",
      "         [-0.2817, -0.1201, -0.4850, -0.5010]],\n",
      "\n",
      "        [[-1.0760, -0.6282, -0.3712, -0.6557],\n",
      "         [-1.2740, -0.7189, -0.3942, -0.8523],\n",
      "         [-1.2174, -0.7141, -0.3821, -0.7969],\n",
      "         [-1.1198, -0.6729, -0.3681, -0.6709],\n",
      "         [-0.8153, -0.4597, -0.4210, -0.6415],\n",
      "         [-0.7520, -0.4396, -0.3840, -0.4388]]], grad_fn=<ViewBackward0>)\n",
      "Shape: torch.Size([2, 6, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = torch.randn(2, NUM_TOKENS, D_IN)\n",
    "\n",
    "print(f\"Input Embeddings:\\n{input_embeddings}\\nShape: {input_embeddings.shape}\\n\")\n",
    "\n",
    "mha = MultiHeadAttention(D_IN, D_OUT, CONTEXT_SIZE, NUM_HEADS, dropout_prob=0.2)\n",
    "output = mha(input_embeddings)\n",
    "\n",
    "print(f\"Multi-Head Attention Output:\\n{output}\\nShape: {output.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addf055e",
   "metadata": {},
   "source": [
    "The last output projection is not necessary but is observed in major LLM architectures. Since the projection weight matrix has size dout x dout, the final output has the same shape of of context vector (B x T x Dout * Dout x Dout -> B x T x Dout)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
