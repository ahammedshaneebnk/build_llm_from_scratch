{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16e6d3c",
   "metadata": {},
   "source": [
    "# Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb4e433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aff33eb",
   "metadata": {},
   "source": [
    "## Causal Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "199fb402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, dropout_prob, context_size):\n",
    "        super(CausalSelfAttention, self).__init__()\n",
    "        self.Wq = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.Wk = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.Wv = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_size, context_size), diagonal=1))\n",
    "\n",
    "    def forward(self, input_embeddings):\n",
    "        num_batch, num_tokens, d_in = input_embeddings.size()\n",
    "        queries = self.Wq(input_embeddings)\n",
    "        keys = self.Wk(input_embeddings)\n",
    "        values = self.Wv(input_embeddings)\n",
    "\n",
    "        dk = keys.shape[-1] # d_out\n",
    "        attention_scores = queries @ keys.transpose(1, 2)\n",
    "\n",
    "        masked_attention_scores = attention_scores.masked_fill(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "\n",
    "        attention_weights = torch.softmax(masked_attention_scores / (dk ** 0.5), dim=-1)\n",
    "        dropped_attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        context_vectors = dropped_attention_weights @ values\n",
    "\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1b7b3",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Wrapper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b2c61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, dropout_prob, context_size):\n",
    "        super(MultiHeadAttentionWrapper, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            CausalSelfAttention(d_in, d_out, dropout_prob, context_size)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_embeddings):\n",
    "        head_outputs = [head(input_embeddings) for head in self.attention_heads]\n",
    "        return torch.cat(head_outputs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "beb4ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_IN = 4\n",
    "D_OUT = 3\n",
    "CONTEXT_SIZE = 6\n",
    "NUM_BATCH = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3cac68a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings:\n",
      "tensor([[[-1.1258, -1.1524, -0.2506, -0.4339],\n",
      "         [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
      "         [ 0.3223, -1.2633,  0.3500,  0.3081],\n",
      "         [ 0.1198,  1.2377,  1.1168, -0.2473],\n",
      "         [-1.3527, -1.6959,  0.5667,  0.7935],\n",
      "         [ 0.5988, -1.5551, -0.3414,  1.8530]],\n",
      "\n",
      "        [[ 0.7502, -0.5855, -0.1734,  0.1835],\n",
      "         [ 1.3894,  1.5863,  0.9463, -0.8437],\n",
      "         [-0.6136,  0.0316, -0.4927,  0.2484],\n",
      "         [ 0.4397,  0.1124,  0.6408,  0.4412],\n",
      "         [-0.1023,  0.7924, -0.2897,  0.0525],\n",
      "         [ 0.5229,  2.3022, -1.4689, -1.5867]]])\n",
      "Shape: torch.Size([2, 6, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "input_embeddings = torch.randn(NUM_BATCH, CONTEXT_SIZE, D_IN)\n",
    "\n",
    "print(f\"Input Embeddings:\\n{input_embeddings}\\nShape: {input_embeddings.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "057eaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HEADS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56ce1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_head_attention = MultiHeadAttentionWrapper(D_IN, D_OUT, NUM_HEADS, dropout_prob=0.2, context_size=CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c47e35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vectors:\n",
      "tensor([[[-1.2488e+00,  6.4150e-01,  1.6728e-01, -1.6614e-01, -1.0396e-01,\n",
      "           5.5464e-02],\n",
      "         [-5.8235e-01,  2.9914e-01,  7.8004e-02, -3.5638e-01,  3.3327e-01,\n",
      "          -1.3804e-01],\n",
      "         [-7.4325e-01,  5.9319e-01,  2.2963e-01, -3.6690e-01,  1.5348e-01,\n",
      "          -1.9105e-01],\n",
      "         [-3.1232e-01,  3.8020e-01,  1.5948e-01, -1.4189e-01,  6.2545e-02,\n",
      "           3.3245e-02],\n",
      "         [-2.8217e-01,  3.3133e-01,  3.8277e-01, -2.1264e-01,  4.9905e-01,\n",
      "          -7.9651e-04],\n",
      "         [ 3.5510e-03,  8.3250e-02, -1.4802e-01, -1.1702e-01,  1.0744e-01,\n",
      "          -6.1177e-02]],\n",
      "\n",
      "        [[-1.1205e-01,  3.2981e-01, -9.5889e-01,  3.1649e-02, -7.8833e-01,\n",
      "          -1.2182e-01],\n",
      "         [ 4.1930e-01,  7.5069e-02, -1.5388e-01, -1.8299e-01,  9.9638e-02,\n",
      "          -1.6085e-01],\n",
      "         [ 3.1157e-01, -1.7752e-01,  3.4384e-01, -4.7404e-02,  8.2824e-02,\n",
      "          -7.6649e-02],\n",
      "         [ 3.7046e-01, -7.2181e-02, -6.9792e-02, -8.3627e-02,  3.4039e-01,\n",
      "          -4.3627e-02],\n",
      "         [ 1.8906e-01, -1.2691e-01,  5.2711e-02,  2.5231e-02,  7.5915e-02,\n",
      "          -2.1377e-02],\n",
      "         [ 2.9251e-01, -2.0937e-01,  1.6434e-01,  9.3087e-02,  8.1876e-02,\n",
      "           2.9340e-02]]], grad_fn=<CatBackward0>)\n",
      "Shape: torch.Size([2, 6, 6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_vectors = multi_head_attention(input_embeddings)\n",
    "\n",
    "print(f\"Context Vectors:\\n{context_vectors}\\nShape: {context_vectors.shape}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
