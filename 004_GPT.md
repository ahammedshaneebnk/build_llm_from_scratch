# How does GPT-3 really work?

## Evolution of the GPT Architecture

The development of the Generative Pre-trained Transformer (GPT) models represents a progression from the original **Transformer** architecture.

* **Transformers (2017):**
    * Based on the paper titled **"Attention Is All You Need"**.
    * This paper introduced the **self-attention mechanism** to capture long-range dependencies in sentences, significantly improving next-word prediction compared to older Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs).
    * The original architecture included both an encoder and a decoder block.
* **GPT (2018):**
    * Introduced the concept of **generative pre-training** using **unsupervised learning**.
    * The key architectural change was removing the encoder block, leaving only the **decoder** block.
    * The model was trained on a large corpus of unlabeled text to predict the next word.
* **GPT-2 (2019):**
    * Based on the paper titled **"Language Models are Unsupervised Multitask Learners"**.
    * This version scaled up the model and data.
    * The largest GPT-2 model had approximately **1 billion parameters**.
* **GPT-3 (2020):**
    * Based on the paper titled **"Language Models are Few-Shot Learners"**.
    * This model featured a massive scale of **175 billion parameters**.
    * It demonstrated exceptional performance across various tasks despite only being trained for next-word prediction.
* **GPT-3.5, GPT-4, etc.:**
    * Subsequent versions continued to increase performance, leading to the current state-of-the-art models.

***

## Learning Paradigms: Zero-Shot vs. Few-Shot

These concepts describe the model's ability to generalize to unseen tasks:

* **Zero-Shot Learning:**
    * The model predicts the answer given **only a task description** and **no prior examples**.
    * *Example:* Prompt: "Translate English to French. Cheese." $\rightarrow$ Output: "Fromage."
* **One-Shot Learning:**
    * The model sees the task description and a **single example** of the task as support.
* **Few-Shot Learning:**
    * The model sees the task description and **a few examples** of the task to guide its response.
    * *Example:* Prompt: *Sea otter $\rightarrow$ loutre de mer. Peppermint $\rightarrow$ menthe poivr√©e.* Translate: *Cheese.* $\rightarrow$ Output: *Fromage.*

The authors of the GPT-3 paper claimed the model was a **few-shot learner**, meaning providing a few examples significantly enhances performance. Modern models like **GPT-4** exhibit strong **zero-shot capabilities** but achieve **more accurate results** through few-shot learning.

***

## Data and Training Scale

The success of GPT-3 is partially attributed to the enormous scale of its training data and compute power:

* **Training Data:**
    * GPT-3 was trained on approximately **300 billion tokens** (words, approximately).
    * **Common Crawl:** 410 billion tokens (60% of the total dataset).
    * **WebText2:** 19 billion tokens (22% of the total dataset, enhanced version of Reddit submissions).
    * **Books and Wikipedia:** The remaining percentage.
* **Architecture Scale:**
    * The GPT-3 architecture used **96 Transformer layers** to accommodate its 175 billion parameters.
* **Pre-training Cost:**
    * The estimated total pre-training cost for GPT-3 was **$4.6 million**.

***

## Generative Pre-training: Unsupervised and Autoregressive

The training process for GPT models is characterized by two key properties:

1.  **Unsupervised Learning (Self-Supervised):**
    * The model does not require external human-provided labels. The task is **Next Word Prediction**, where the **structure of the sentence itself** is used to create the training data and labels (the true next word serves as the label).
2.  **Auto-Regressive Model:**
    * This means the **output from a previous step is used as the input for the next step**. In each iteration, the newly predicted word is appended to the input sequence and fed back into the model to predict the word that follows.

This process involves optimizing the 175 billion parameters to predict the next word across billions of data points, requiring massive compute power.

***

## Emergent Behavior

**Emergent behavior** is the **ability of a large language model to perform tasks that it was not explicitly trained to perform**.

* Despite being trained only for next-word prediction, GPT models develop capabilities for complex tasks such as:
    * Language translation
    * Generating multiple-choice questions (MCQs)
    * Summarization
    * Essay grading
* This unexpected capacity to generalize and perform novel tasks is a key area of active research.

***

## Summary

* **Evolution:** GPT architecture simplified the Transformer by removing the **encoder** block, relying solely on the **decoder**. Key papers were **"Attention Is All You Need"** (Transformer) and **"Language Models are Few-Shot Learners"** (GPT-3).
* **Scale:** GPT-3 achieved its capabilities using **175 billion parameters** and training on approximately **300 billion tokens** of text.
* **Pre-training:** The training is **unsupervised** (using next-word prediction) and **auto-regressive** (feeding previous outputs back as future inputs).
* **Learning:** Models like GPT-3/4 perform well in a **zero-shot** setting but are often more accurate in a **few-shot** setting.
* **Capabilities:** The model's ability to perform tasks it wasn't explicitly trained for, such as translation or grading, is known as **emergent behavior**.