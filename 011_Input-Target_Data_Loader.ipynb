{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda2ad90",
   "metadata": {},
   "source": [
    "# Read Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c3315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab6e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_DIR = \"Resources\"\n",
    "HARRY_POTTER_SS_FILE = \"Harry_Potter_and_Sorcerer's_Stone.txt\"\n",
    "FILE_PATH = os.path.join(RESOURCE_DIR, HARRY_POTTER_SS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4932a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE_PATH, 'r', encoding='windows-1252') as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ff6a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 442745 characters\n",
      "First 200 characters:\n",
      "Harry Potter and the Sorcerer's Stone \n",
      "\n",
      "CHAPTER ONE \n",
      "\n",
      "THE BOY WHO LIVED \n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. Th\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of text: {len(raw_text)} characters\")\n",
    "print(f\"First 200 characters:\\n{raw_text[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bcffbf",
   "metadata": {},
   "source": [
    "# Input-Target Creation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a124f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1384f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b562338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 113794\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(raw_text)\n",
    "print(f\"Total number of tokens: {len(token_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0069a69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 20 token IDs: [18308, 14179, 290, 262, 30467, 338, 8026, 220, 198, 198, 41481, 16329, 220, 198, 198, 10970, 16494, 56, 19494, 406]\n",
      "Tokens for first 20 IDs: ['Harry', ' Potter', ' and', ' the', ' Sorcerer', \"'s\", ' Stone', ' ', '\\n', '\\n', 'CHAPTER', ' ONE', ' ', '\\n', '\\n', 'THE', ' BO', 'Y', ' WHO', ' L']\n"
     ]
    }
   ],
   "source": [
    "print(f\"first 20 token IDs: {token_ids[:20]}\")\n",
    "print(f\"Tokens for first 20 IDs: {[tokenizer.decode([id]) for id in token_ids[:20]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbfe541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens:\n",
      "[' We', \"'re\", ' going', ' away', '.', ' Just', ' pack', ' some', ' clothes', '.', ' No', ' arguments', '!\"', ' ', '\\n', '\\n', 'He', ' looked', ' so', ' dangerous', ' with', ' half', ' his', ' mustache', ' missing', ' that', ' no', ' one', ' dared', ' argue']\n"
     ]
    }
   ],
   "source": [
    "sample_token_ids = token_ids[15005:]\n",
    "sample_tokens = [tokenizer.decode([id]) for id in sample_token_ids]\n",
    "print(f\"Sample tokens:\\n{sample_tokens[:30]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab6dfd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs for 'Harry': [18308]\n",
      "Decoded tokens for IDs [18308]: ['Harry']\n"
     ]
    }
   ],
   "source": [
    "test = \"Harry\"\n",
    "test_ids = tokenizer.encode(test)\n",
    "print(f\"Token IDs for '{test}': {test_ids}\")\n",
    "print(f\"Decoded tokens for IDs {test_ids}: {[tokenizer.decode([id]) for id in test_ids]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8d8f62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token IDs (x0): [775, 821, 1016, 1497]\n",
      "Target token IDs (y0): [821, 1016, 1497, 13]\n",
      "Input tokens (x0): [' We', \"'re\", ' going', ' away']\n",
      "Target tokens (y0): [\"'re\", ' going', ' away', '.']\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x0 = sample_token_ids[0:context_size]\n",
    "y0 = sample_token_ids[1:context_size+1]\n",
    "\n",
    "print(f\"Input token IDs (x0): {x0}\")\n",
    "print(f\"Target token IDs (y0): {y0}\")\n",
    "\n",
    "print(f\"Input tokens (x0): {[tokenizer.decode([id]) for id in x0]}\")\n",
    "print(f\"Target tokens (y0): {[tokenizer.decode([id]) for id in y0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42304a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 predictions per pair (input, target)\n",
      "[775] ---> 821\n",
      "[775, 821] ---> 1016\n",
      "[775, 821, 1016] ---> 1497\n",
      "[775, 821, 1016, 1497] ---> 13\n",
      "\n",
      "4 predictions per pair (input tokens, target token)\n",
      "[' We'] ---> 're\n",
      "[' We', \"'re\"] --->  going\n",
      "[' We', \"'re\", ' going'] --->  away\n",
      "[' We', \"'re\", ' going', ' away'] ---> .\n"
     ]
    }
   ],
   "source": [
    "print(f\"{context_size} predictions per pair (input, target)\")\n",
    "for i in range(context_size):\n",
    "    input = x0[:i+1]\n",
    "    target = y0[i]\n",
    "    print(f\"{input} ---> {target}\")\n",
    "\n",
    "print(f\"\\n{context_size} predictions per pair (input tokens, target token)\")\n",
    "for i in range(context_size):\n",
    "    input_tokens = [tokenizer.decode([id]) for id in x0[:i+1]]\n",
    "    target_token = tokenizer.decode([y0[i]])\n",
    "    print(f\"{input_tokens} ---> {target_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e9526e",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17b14b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # pip install torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d817a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, raw_text, tokenizer, context_size, stride):\n",
    "        self.input_token_ids = []\n",
    "        self.target_token_ids = []\n",
    "\n",
    "        all_token_ids = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"}) # allowed_special is a set\n",
    "\n",
    "        for i in range(0, len(all_token_ids) - context_size, stride):\n",
    "            input_chunk = all_token_ids[i : i + context_size]\n",
    "            target_chunk = all_token_ids[i + 1 : i + context_size + 1]\n",
    "            self.input_token_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_token_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_token_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_token_ids[idx]\n",
    "        y = self.target_token_ids[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b0cca",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "394f3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "298d7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(raw_text, tokenizer, context_size=256, stride=256, batch_size=8,\n",
    "                         shuffle=True, num_workers=0, drop_last=True):\n",
    "    \n",
    "    dataset = GPTDatasetV1(raw_text, tokenizer, context_size, stride)\n",
    "    \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, num_workers=num_workers, drop_last=drop_last)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7540e",
   "metadata": {},
   "source": [
    "## Test Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "828151a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = create_dataloader_v1(raw_text, tokenizer, context_size=4, stride=1, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31e602",
   "metadata": {},
   "source": [
    "* Use Python's builtin **iter** and **next**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95b56581",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43e37413",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a48292f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch\n",
      "[tensor([[18308, 14179,   290,   262]]), tensor([[14179,   290,   262, 30467]])]\n"
     ]
    }
   ],
   "source": [
    "print(f\"First batch\\n{first_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee26c5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second batch\n",
      "[tensor([[14179,   290,   262, 30467]]), tensor([[  290,   262, 30467,   338]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "\n",
    "print(f\"Second batch\\n{second_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65791f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[18308, 14179,   290,   262],\n",
      "        [30467,   338,  8026,   220],\n",
      "        [  198,   198, 41481, 16329],\n",
      "        [  220,   198,   198, 10970],\n",
      "        [16494,    56, 19494,   406],\n",
      "        [ 3824,  1961,   220,   198],\n",
      "        [  198,  5246,    13,   290],\n",
      "        [ 9074,    13,   360,  1834]])\n",
      "\n",
      "targets: tensor([[14179,   290,   262, 30467],\n",
      "        [  338,  8026,   220,   198],\n",
      "        [  198, 41481, 16329,   220],\n",
      "        [  198,   198, 10970, 16494],\n",
      "        [   56, 19494,   406,  3824],\n",
      "        [ 1961,   220,   198,   198],\n",
      "        [ 5246,    13,   290,  9074],\n",
      "        [   13,   360,  1834,  1636]])\n"
     ]
    }
   ],
   "source": [
    "data_loader2 = create_dataloader_v1(raw_text, tokenizer, context_size=4, stride=4, batch_size=8, shuffle=False)\n",
    "\n",
    "data_iter2 = iter(data_loader2)\n",
    "first_batch2 = next(data_iter2)\n",
    "\n",
    "first_batch2_inputs, first_batch2_targets = first_batch2\n",
    "print(f\"inputs: {first_batch2_inputs}\")\n",
    "print(f\"\\ntargets: {first_batch2_targets}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
